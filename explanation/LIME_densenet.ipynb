{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "import types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the folder with chexpert dataset which has structure outline in the project description\n",
    "data_path = '/home/tu-serbin/data'\n",
    "\n",
    "train_csv_path = data_path + '/chexpert/v1.0/train.csv'\n",
    "valid_csv_path = data_path + '/chexpert/v1.0/valid.csv'\n",
    "dir_path = data_path + '/chexpert/v1.0/'\n",
    "\n",
    "#path to the directory with saved state dictionaries\n",
    "model_save_dir = '/home/tu-serbin/group/igloo/alex/saves/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that drops lateral image records and irrelevant columns, replaces -1 with 0 and edits the \"Path\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropper(df):\n",
    "    d = df.copy()\n",
    "    index = d[d[\"Frontal/Lateral\"] == \"Lateral\"].index\n",
    "    d.drop(index=index, axis=0, inplace=True)\n",
    "    d = d.drop(columns=['Sex','Age','Frontal/Lateral','AP/PA'])\n",
    "    d = d.replace(-1.0,0)\n",
    "    d.Path = d.Path.str.replace('CheXpert-','chexpert/')\n",
    "    d = d.reset_index(drop=True)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_csv = pd.read_csv(valid_csv_path, sep=',').fillna(0)\n",
    "dval = dropper(valid_csv)\n",
    "\n",
    "train_csv = pd.read_csv(train_csv_path, sep=',').fillna(0)\n",
    "dtrain = dropper(train_csv)\n",
    "\n",
    "index_ = dval.sum().drop('Path') < 5\n",
    "bad_cols = dval.drop(columns='Path').columns[index_.to_list()]\n",
    "good_cols = dval.columns.drop(bad_cols.to_list()+['Path'])\n",
    "\n",
    "# image paths as Series\n",
    "vpath = dval.Path\n",
    "tpath = dtrain.Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnet_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DenseNet-169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DenseNet121 \n",
    "model = torchvision.models.densenet121()\n",
    "# Get the input dimension of last layer\n",
    "kernel_count = model.classifier.in_features\n",
    "# Replace last layer with new layer that have num_classes nodes, after that apply Sigmoid to the output\n",
    "model.classifier = nn.Sequential(nn.Linear(kernel_count, 14), nn.Sigmoid())\n",
    "\n",
    "name = 'epoch_1_score_0.81652.pth'\n",
    "model.load_state_dict(torch.load(os.path.join(model_save_dir, name))['state_dict'])\n",
    "_=model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for generating LIME images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lime_explainer():\n",
    "    def __init__(self, mod):\n",
    "        self.model = mod\n",
    "        \n",
    "    def explain_please(self, im:Image.Image, transform=None, cuda=False):\n",
    "        im_r = im\n",
    "        \n",
    "        self.c = cuda\n",
    "            \n",
    "        if transform:\n",
    "            im_r = transform(im_r)\n",
    "            \n",
    "        #print('image shape', im_r.shape) #3,224,224\n",
    "        self.model.eval()     \n",
    "        #print(self.model(im_r.unsqueeze(0)))\n",
    "              \n",
    "        explainer = lime_image.LimeImageExplainer()\n",
    "        explanation = explainer.explain_instance(np.transpose(im_r.numpy(),(1,2,0)).astype(np.double), #resized image as numpy array \n",
    "                                         self.predictor, # classification function\n",
    "                                         top_labels=3, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000) # number of images that will be sent to classification function\n",
    "        return explanation\n",
    "    \n",
    "    def predictor(self, im:np.array):\n",
    "        #from IPython.core.debugger import Tracer; Tracer()() \n",
    "        l = im.shape[0]\n",
    "        im1 = np.transpose(im, (0,3,1,2))\n",
    "        im1 = torch.Tensor(im1)\n",
    "        \n",
    "        logits = self.model(im1)       \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return probs.cpu().detach().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which takes image path and produces image segmentation using LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnet_explain(impath):\n",
    "    \n",
    "    image = Image.open(impath).convert('RGB')\n",
    "\n",
    "    lime_ = lime_explainer(model)\n",
    "    \n",
    "    exp = lime_.explain_please(image, dnet_transform, cuda=False)\n",
    "\n",
    "    temp, mask = exp.get_image_and_mask(exp.top_labels[0], positive_only=False, num_features=10,\\\n",
    "                                        hide_rest=False)\n",
    "    img_boundry1 = mark_boundaries(temp, mask)\n",
    "    \n",
    "    plt.imshow(img_boundry1)\n",
    "    \n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick random image and run LIME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5cb24b878e4a5aa1e92e08ef02795f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_id = np.random.randint(dtrain.shape[0])\n",
    "path1 = data_path+dtrain.Path[image_id]\n",
    "\n",
    "dnet_explain(path1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
